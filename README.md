## Fondations — Transformers et LLMs

- **Ashish Vaswani et al. (2017).**  
    Attention Is All You Need._ NeurIPS.  
    
- **Jacob Devlin et al. (2018).**  
    BERT: Pre-training of Deep Bidirectional Transformers._ NAACL.  
    
- **Tom B. Brown et al. (2020).**  
    Language Models are Few-Shot Learners._ NeurIPS.
  
## Instruction-Tuning & Alignement

- **Jason Wei et al. (2022).**  
    Finetuned Language Models Are Zero-Shot Learners._ ICLR.  
    
- **Long Ouyang et al. (2022).**  
    Training Language Models to Follow Instructions with Human Feedback._ NeurIPS.  
    
## In-Context Learning

- **Sewon Min et al. (2022).**  
    Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?_ EMNLP.


## PEFT — Adapters & LoRA
    
- **Edward J. Hu et al. (2021).**  
    LoRA: Low-Rank Adaptation of Large Language Models._ ICLR.  
    
## Retrieval-Augmented Generation (RAG)

- **Patrick Lewis et al. (2020).**  
    Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks._ NeurIPS.
    
- **Vladimir Karpukhin et al. (2020).**  
    Dense Passage Retrieval for Open-Domain Question Answering._ EMNLP.

## QA Datasets

- **Pranav Rajpurkar et al. (2016).**  
    SQuAD: 100,000+ Questions for Machine Comprehension of Text._ EMNLP.


# Résumé automatique

- **Chin-Yew Lin (2004).**  
    ROUGE: A Package for Automatic Evaluation of Summaries._ ACL Workshop.  


- **Tom B. Brown et al. (2020).**  
    Language Models are Few-Shot Learners
    
- **Jason Wei et al. (2022).**  
    Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
    
- **OpenAI (2022).**  
    Training language models to follow instructions with human feedback

- **Alexander R. Fabbri et al. (2019).**
	Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model]
	
- **Congbo Ma et al. (2023).**
	Multi-document Summarization via Deep Learning Techniques: A Survey.
